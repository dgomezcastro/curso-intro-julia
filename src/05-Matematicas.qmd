---
title: "![Julia](Material/Julia.svg)"
subtitle: "Paquetes de cálculo científico"
author: "David Gómez-Castro"
format:
  clean-revealjs:
    output-file: 05-Matematicas-diapositivas.html
  html:
    output-file: 05-Matematicas-apuntes.html
---

:::{.hidden}
```{julia}
using Pkg
Pkg.activate(".")
```
:::

Entre otras ventajas, los algoritmos matemáticos en `julia` pueden quedar escritos de manera muy agradable

[https://github.com/mossr/BeautifulAlgorithms.jl](https://github.com/mossr/BeautifulAlgorithms.jl)

# `LinearAlgebra.jl` 

Es parte de `Standard Library`

## Operaciones básicas {.scrollable}

```{julia}

using LinearAlgebra
A = [1 2 3; 4 1 6; 7 8 1]
```

```{julia}

@show tr(A)
@show det(A)
println("inv(A) = ")
inv(A)
```

La sintaxis para sistemas es la habitual
```{julia}
b = [1,2,3]
A \ b 
```

. . .


Y las descomposición espectral
```{julia}
eigvals(A) 
```

## Matrices especiales 

| Type | 	Description | 
|------|-----------------
|Symmetric|	Symmetric matrix|
|Hermitian|	Hermitian matrix|
|UpperTriangular|	Upper triangular matrix|
|UnitUpperTriangular|	Upper triangular matrix with unit diagonal|
|LowerTriangular|	Lower triangular matrix|
|UnitLowerTriangular|	Lower triangular matrix with unit diagonal|
|UpperHessenberg|	Upper Hessenberg matrix|
|Tridiagonal|	Tridiagonal matrix|
|SymTridiagonal|	Symmetric tridiagonal matrix|
|Bidiagonal|	Upper/lower bidiagonal matrix|
|Diagonal|	Diagonal matrix|
|UniformScaling|	Uniform scaling operator|

--- 

```{julia}
A = UnitUpperTriangular( [ 1 2 3 ; 1 2 3 ; 1 2 3 ] )
```

[Más sobre matrices especiales](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#Special-matrices)

## Factorizaciones {.scrollable}

`lu` permite exportar L, U y vector de permutaciones, o matriz de permutaciones
```{julia} 
A = rand(3,3)
B = lu(A)
```

. . . 

Julia incluso puede elegir una "buena" factorización para nuestro problema
```{julia}
C = factorize(A)
```

```{julia}
D = factorize(A'*A)
```

Volveremos sobre la importancia de factorizar en la próxima sesión.

## Más sobre sistemas lineales

Para sistemas "difíciles" o cuando queramos la máxima velocidad, podemos utilizar el paquete `LinearSolve.jl`
```{julia}
using LinearSolve

A = rand(4, 4)
b = rand(4)
prob = LinearProblem(A, b)
sol = solve(prob)
sol.u
```

Implementa muchos resolvedores optimizados y tiene, además, un sistema de selección de solvers adecuados.

Video explicativo en [Youtube](https://www.youtube.com/watch?v=JWI34_w-yYw&pp=ygUOcmF1Y2thdXMganVsaWE%3D)



# Estadística y teoría de la medida

[Random.jl](https://docs.julialang.org/en/v1/stdlib/Random/) es parte de `Standard Library`

```{julia}
using Random 
@show rand()
@show rand( (-1,2) )
@show rand(5:10);
```

. . .

[Statistics.jl](https://docs.julialang.org/en/v1/stdlib/Statistics/) es parte de Standard Library.

. . .

Es reseñable la familia [JuliaStats](https://juliastats.org/): DataFrames, Distributions, HypothesisTests, TimeSeries, ... 

## [Distributions.jl](https://juliastats.org/Distributions.jl/stable/)

The Distributions package provides a large collection of probabilistic distributions and related functions. Particularly, Distributions implements:

* Sampling from distributions
* Moments (e.g mean, variance, skewness, and kurtosis), entropy, and other properties
* Probability density/mass functions (pdf) and their logarithm (logpdf)
* Moment-generating functions and characteristic functions
* Maximum likelihood estimation
* Distribution composition and derived distributions (Cartesian product of distributions, truncated distributions, censored distributions)

--- 

```{julia}
using Random, Distributions
d = Normal()
d + 1.0
```

```{julia}
@show mean(d)
@show rand(d, 3)
```

```{julia}
x = [0, 0.4, -0.4] 
fit(Normal, x)
```


Ver también [MeasureTheory.jl](https://cscherrer.github.io/MeasureTheory.jl/stable/)


## [Turing.jl](https://turinglang.org/dev/docs/using-turing/)

> Turing is a general-purpose probabilistic programming language for robust, efficient Bayesian inference and decision making. Current features include:

|   * General-purpose probabilistic programming with an intuitive modelling interface;
|   * Robust, efficient Hamiltonian Monte Carlo (HMC) sampling for differentiable posterior distributions;
|   * Particle MCMC sampling for complex posterior distributions involving discrete variables and stochastic control flow; and
|   * Compositional inference via Gibbs sampling that combines particle MCMC, HMC and random-walk MH (RWMH).

[Un ejemplo sencillo](https://turinglang.org/dev/docs/using-turing/quick-start)

# [`Plots.jl`](https://docs.juliaplots.org/latest/)

## [Tutorial básico](https://docs.juliaplots.org/latest/tutorial/)

## Creando animaciones 

```{julia}
#| eval: false

using Plots 
anim = @animate for n=1:20
    plot( x-> sin(n*x) , label="")
end;

gif(anim, "animation.gif",  fps = 30)

mp4(anim, "video.mp4",  fps = 30)
```

--- 

En un notebook se puede hacer
```{julia}
using Plots
@gif for n=1:20
    plot( x-> sin(n*x) , label="" )
end
```

## Pintando polígonos

```{julia}
x,y =  [0.0, 0.5, 1.0],  [0.0, 1.0, 0.0]
plot(x[[1:end;1]] , y[[1:end; 1]], fill = 0)
```

## Otras librerías de representación

- [PyPlot.jl](https://github.com/JuliaPy/PyPlot.jl)

- [Makie.jl](https://github.com/MakieOrg/Makie.jl)


# [SciML](https://docs.sciml.ai/Overview/stable/)

 SciML is the combination of scientific computing techniques with machine learning

![](Material/sciml.svg)


## [Ecuaciones Diferenciales](https://docs.sciml.ai/DiffEqDocs/stable/)

[Resolución de EDOs](https://docs.sciml.ai/DiffEqDocs/stable/getting_started/)

```{julia}
using DifferentialEquations
f(u,p,t) = 1.01*u
u0 = 1/2
tspan = (0.0,1.0)
prob = ODEProblem(f,u0,tspan)
sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)

using Plots
plot(sol,linewidth=5,title="Solution to the linear ODE with a thick line",
     xaxis="Time (t)",yaxis="u(t) (in μm)",label="My Thick Line!") # legend=false
plot!(sol.t, t->0.5*exp(1.01t),lw=3,ls=:dash,label="True Solution!")
```

## [ModelingToolkit.jl](https://docs.sciml.ai/ModelingToolkit/stable/)

> ModelingToolkit.jl is a modeling language for high-performance symbolic-numeric computation in scientific computing and scientific machine learning. It then mixes ideas from symbolic computational algebra systems with causal and acausal equation-based modeling frameworks to give an extendable and parallel modeling system. It allows for users to give a high-level description of a model for symbolic preprocessing to analyze and enhance the model. Automatic symbolic transformations, such as index reduction of differential-algebraic equations, make it possible to solve equations that are impossible to solve with a purely numeric-based technique.

## `ModelingToolkit.jl` una capa más sobre DifferentialEquations

```{julia} 
using ModelingToolkit
using ModelingToolkit: t_nounits as t, D_nounits as D

@mtkmodel FOL begin
    @parameters begin
        τ = 3.0 # parameters
    end
    @variables begin
        x(t) = 0.0 # dependent variables
    end
    @equations begin
        D(x) ~ (1 - x) / τ
    end
end

using OrdinaryDiffEq
@mtkbuild fol = FOL()
prob = ODEProblem(fol, [], (0.0, 10.0), [])
sol = solve(prob)

using Plots
plot(sol)
```

--- 

[Vídeo con un ejemplo industrial](https://www.youtube.com/watch?v=ChwKqrH8OQU)

## [JumpProcesses.jl](https://docs.sciml.ai/JumpProcesses/stable/)

## [FeNICS.jl](https://docs.sciml.ai/FEniCS/stable/)

> FEniCS.jl is a wrapper for the FEniCS library for finite element discretizations of PDEs. This wrapper includes three parts:

> Installation and direct access to FEniCS via a Conda installation. Alternatively one may use their current FEniCS installation.
A low-level development API and provides some functionality to make directly dealing with the library a little bit easier, but still requires knowledge of FEniCS itself. Interfaces have been provided for the main functions and their attributes, and instructions to add further ones can be found here.
A high-level API for usage with DifferentialEquations. An example can be seen solving the heat equation with high order adaptive timestepping.

# Ecuaciones en Derivadas Parciales

## [Gridap.jl](https://github.com/gridap/Gridap.jl)

> Gridap provides a set of tools for the grid-based approximation of partial differential equations (PDEs) written in the Julia programming language. The main motivation behind the development of this library is to provide an easy-to-use framework for the development of complex PDE solvers in a dynamically typed style without sacrificing the performance of statically typed languages. The library currently supports linear and nonlinear PDE systems for scalar and vector fields, single and multi-field problems, conforming and nonconforming finite element discretizations, on structured and unstructured meshes of simplices and hexahedra.

It has a very reach [library of worked examples](https://gridap.github.io/Tutorials/dev/pages/t001_poisson/)


## [JuliaFEM](http://www.juliafem.org/JuliaFEM.jl/stable/)

The JuliaFEM project develops open-source software for reliable, scalable, distributed Finite Element Method.


# Optimización: [JuMP.jl](https://jump.dev/JuMP.jl/stable/)

> JuMP is a domain-specific modeling language for mathematical optimization embedded in Julia. It currently supports a number of open-source and commercial solvers for a variety of problem classes, including linear, mixed-integer, second-order conic, semidefinite, and nonlinear programming.

> JuMP is a package for Julia. From Julia, JuMP is installed by using the built-in package manager.

```{julia} 
#| eval: false 
import Pkg
Pkg.add("JuMP")
```

> You also need to include a Julia package which provides an appropriate solver. One such solver is HiGHS.Optimizer, which is provided by the HiGHS.jl package.

```{julia} 
#| eval: false 
import Pkg
Pkg.add("HiGHS")
```

## [Ejemplo](https://jump.dev/JuMP.jl/stable/tutorials/getting_started/getting_started_with_JuMP/#Step-by-step)

## Limitaciones y alternativas

> Even if your problem is differentiable, if it is unconstrained there is limited benefit (and downsides in the form of more overhead) to using JuMP over tools which are only concerned with function minimization.

## Otras opciones

* [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl)
* [Optimization.jl](https://optimization.sciml.ai/stable/)
* [NLopt.jl](https://github.com/JuliaOpt/NLopt.jl)

# Cálculo simbólico

## [Symbolics.jl](https://symbolics.juliasymbolics.org/dev/)

```{julia}
using Symbolics
@variables x y
```

. . .

```{julia}
z = x^2 + y
```

. . .

```{julia}
simplify(2x + 2y)
```

### Derivadas

```{julia}
@variables x g(x)
Dt = Differential(x)
```

. . .

```{julia}
expression = expand_derivatives(Dt(x*g))
```

. . .

```{julia}
expr2 = substitute(expression, Dict([g => x]))
```

. . .

```{julia}
expand_derivatives( expr2 )
```

### Limitaciones

There is a list available of [known missing features](https://github.com/JuliaSymbolics/Symbolics.jl/issues/59).

## [SymPy.jl](http://sympy.org/)

> SymPy is a Python library for symbolic mathematics.

This package is a wrapper using PyCall

> With the excellent PyCall package of julia, one has access to the many features of the SymPy library from within a Julia session.

It requires Python to run in the background. To install it sometimes it is needed to make a few 
```{julia}
#| eval: false 
using Pkg
Pkg.add("Conda") #  if needed
using Conda
Conda.update()
```

> The only point to this package is that using PyCall to access SymPy is somewhat cumbersome.

## [Symbolics.jl vs Sympy.jl](https://symbolics.juliasymbolics.org/dev/comparison/)

# Redes neuronales

![https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg](Material/Colored_neural_network.svg)


## Una capa
		
::: {.column width="50%"}
![https://commons.wikimedia.org/wiki/File:Single_layer_ann.svg](Material/Single_layer_ann.svg)
:::


::: {.column width="40%"}

* $x_1, x_2, \cdots$ son las entradas
					
* $y_1, y_2, \cdots$ son las salidas
					
* $w_{11}, w_{12}, \cdots$ son los \textit{pesos}
					
*	Se calcula $\displaystyle b_1 = w_{11} x_1 + w_{21} x_2 + w_{31} x_3 + \cdots$
					
*	Y luego $y_1 = f ( b_1 )$ 

* $f$ es la llamada \textit{función de activación}

::: 
		
Denotamos  $\mathbf x = (x_1, \cdots, x_p)$, $\mathbf y = (y_1, \cdots, y_q)$, $\mathbf W = (w_{11}, w_{12}, \cdots )$

## Red de neuronas

::: {.column width="50%"}
Se pueden hacer redes 

de varias capas: 

la salida de una 

es la entrada de la otra.


Si tenemos muchos pares de entradas y sus respectivas salidas 

$$
  \widehat{\mathbf y_i} = F(\mathbf x_i, \mathbf W)
$$

Buscar los mejores pesos $\mathbf W$ se llama

***entrenar la red***
::: 

::: {.column width="40%"}
![](Material/Colored_neural_network.svg)
::: 

Hay muchas generalizaciones de esta idea: redes profundas (muchas capas), redes convolucionales, ...

¿Cómo de potente es esto? [Vídeo de Google DeepMind](https://www.youtube.com/watch?v=imOt8ST4Ejc)

## Evaluación de la red: forward-propagation

Al algoritmo para, dados los pesos y la entrada calcular la salida se lo llama forward propagation.

## Entrenamiento de redes

Se separan los datos en un conjunto de entrenamiento y conjunto de pruebas

. . .

Para ajustar la red tenemos que elegir una función de pérdida $d({\mathbf y}, \widehat{\mathbf y})$. Por ejemplo $|{\mathbf y} - \widehat{\mathbf y}|^2$.

. . .

A partir de ella elegimos una función de coste, que tenga en cuenta todos las pérdidas. Por ejemplo el error cuadrático medio
$$
  J(W) = \frac{1}{N} \sum_{i=1}^N  |{\mathbf y_i} - \widehat{\mathbf y_i}|^2
$$

. . .

Así:

* Se elige $\mathbf W$ minimizando el coste sobre el conjunto de entrenamiento
* Se comprueba que es "bueno" en general el coste con este $\mathbf W$ sobre el conjunto de prueba (que debe ser distinto al de entrenamiento)

Hay múltiples opciones para la elección de función de coste y hacer la optimización eficientemente es gran parte de la dificultad.

## Optimización en el entrenamiento

Para optimizar se utilizan múltiples procedimientos. 

El ejemplo más sencillo es el descenso por gradiente 
$$
  W_{n+1} = W_n - \gamma \Delta J (W_n).
$$ 

Para calcular $\nabla J$ se emplea la regla de cadena. A este proceso lo llama back-progation.

# [Un ejemplo detallado en Julia](https://towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8)

# [Flux.jl](https://fluxml.ai/Flux.jl/stable/)

Flux es una de las mejores librerías de ML en Julia.

[Video de introducción](https://www.youtube.com/watch?v=R81pmvTP_Ik)

## [Ajustar una línea](https://fluxml.ai/Flux.jl/stable/models/overview/)

## [Un ejemplo no trivial](https://fluxml.ai/Flux.jl/stable/models/quickstart/)

# Otros paquetes

[MLJ.jl](https://juliapackages.com/p/mlj)