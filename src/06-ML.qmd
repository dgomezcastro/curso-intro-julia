---
title: "![Julia](Material/Julia.svg)"
subtitle: "Redes neuronales"
author: "David Gómez-Castro"
format:
  clean-revealjs:
    output-file: 06-slides.html
---

:::{.hidden}
```{julia}
using Pkg
Pkg.activate(".")
```
:::

# Redes neuronales

![https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg](Material/Colored_neural_network.svg)


## Una capa
		
::: {.column width="50%"}
![https://commons.wikimedia.org/wiki/File:Single_layer_ann.svg](Material/Single_layer_ann.svg)
:::


::: {.column width="40%"}

* $x_1, x_2, \cdots$ son las entradas
					
* $y_1, y_2, \cdots$ son las salidas
					
* $w_{11}, w_{12}, \cdots$ son los \textit{pesos}
					
*	Se calcula $\displaystyle b_1 = w_{11} x_1 + w_{21} x_2 + w_{31} x_3 + \cdots$
					
*	Y luego $y_1 = f ( b_1 )$ 

* $f$ es la llamada \textit{función de activación}

::: 
		
Denotamos  $\mathbf x = (x_1, \cdots, x_p)$, $\mathbf y = (y_1, \cdots, y_q)$, $\mathbf W = (w_{11}, w_{12}, \cdots )$

## Red de neuronas

::: {.column width="50%"}
Se pueden hacer redes 

de varias capas: 

la salida de una 

es la entrada de la otra.


Si tenemos muchos pares de entradas y sus respectivas salidas 

$$
  \widehat{\mathbf y_i} = F(\mathbf x_i, \mathbf W)
$$

Buscar los mejores pesos $\mathbf W$ se llama

***entrenar la red***
::: 

::: {.column width="40%"}
![](Material/Colored_neural_network.svg)
::: 

Hay muchas generalizaciones de esta idea: redes profundas (muchas capas), redes convolucionales, ...

¿Cómo de potente es esto? [Vídeo de Google DeepMind](https://www.youtube.com/watch?v=imOt8ST4Ejc)

## Evaluación de la red: forward-propagation

Al algoritmo para, dados los pesos y la entrada calcular la salida se lo llama forward propagation.

## Entrenamiento de redes

Se separan los datos en un conjunto de entrenamiento y conjunto de pruebas

. . .

Para ajustar la red tenemos que elegir una función de pérdida $d({\mathbf y}, \widehat{\mathbf y})$. Por ejemplo $|{\mathbf y} - \widehat{\mathbf y}|^2$.

. . .

A partir de ella elegimos una función de coste, que tenga en cuenta todos las pérdidas. Por ejemplo el error cuadrático medio
$$
  J(W) = \frac{1}{N} \sum_{i=1}^N  |{\mathbf y_i} - \widehat{\mathbf y_i}|^2
$$

. . .

Así:

* Se elige $\mathbf W$ minimizando el coste sobre el conjunto de entrenamiento
* Se comprueba que es "bueno" en general el coste con este $\mathbf W$ sobre el conjunto de prueba (que debe ser distinto al de entrenamiento)

Hay múltiples opciones para la elección de función de coste y hacer la optimización eficientemente es gran parte de la dificultad.

## Optimización en el entrenamiento

Para optimizar se utilizan múltiples procedimientos. 

El ejemplo más sencillo es el descenso por gradiente 
$$
  W_{n+1} = W_n - \gamma \Delta J (W_n).
$$ 

Para calcular $\nabla J$ se emplea la regla de cadena. A este proceso lo llama back-progation.

# [Un ejemplo detallado en Julia](https://towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8)

# [Flux.jl](https://fluxml.ai/Flux.jl/stable/)

Flux es una de las mejores librerías de ML en Julia.

[Video de introducción](https://www.youtube.com/watch?v=R81pmvTP_Ik)

## [Ajustar una línea](https://fluxml.ai/Flux.jl/stable/models/overview/)

## [Un ejemplo no trivial](https://fluxml.ai/Flux.jl/stable/models/quickstart/)

# Otros paquetes

[MLJ.jl](https://juliapackages.com/p/mlj)